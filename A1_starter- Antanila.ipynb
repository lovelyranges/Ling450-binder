{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a3f7de",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. \n",
    "The binder contains this notebook and some sample files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eaff9e",
   "metadata": {},
   "source": [
    "•\tDescribe the approach. What does the code do? \t\t\t\t\t\t\n",
    "•\tDescribe the output. Does it do what you expected?\t\t\t\t\t\n",
    "•\tDescribe the output given the task. Does it capture all the direct quotes in the text? Why or why not? Does it capture things that are not direct quotes? Why or why not? \n",
    "•\tIf not all the direct quotes are captured, what is missing and how could you fix it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155d2d3",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e6937",
   "metadata": {},
   "source": [
    "At first, we import spacy, which is a natural language process (nlp) object which is a function to analyze the given text. Then we import re 'regular expression' which is a function which matches a particular string with the regular expression, usually it comes back as the same. The second code with open (\"\") as f is telling the system to return a file which we are going to read and analyze. When returning file we input encoding in order to encode or decode the file since some files have to be read as a specific encoding type. 'r' is for reading the file that exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "077c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "009437e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e55d17",
   "metadata": {},
   "source": [
    "def is used to define a function in this case find_sents is returning sentences ffrom the text that is being retrieved by the file above. Then we create a doc object by processing a string of text with the nlp object and in this case we have chosen the file \"data/5c1dbe1d1e67d78e2797d611.txt\". We attempt to call the read() method for the variable file outside the with statementThe code will return with a list of sentences from the document. Then we use spacy.load to load en_core_web_sm is a trained pipline which enables spaCy to predict linguistics attriutes in context such as syntactic dependencies, named entities and parts of speech tags. However, this code is not going to process the text, it will only pull sentences from the text. The list(doc.sents) is a property used to iterate over the sentences in the document \"data/5c1dbe1d1e67d78e2797d611.txt\" we are analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a9f3f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02787d49",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb58ca",
   "metadata": {},
   "source": [
    "findall() function is extremely useful as it returns a list of strings containing all matches. the code set the pattern to ‘“(.+?)”’, where the single quotes represent the text body, while the double quotes represent the quotes within the text. We have the parenthesis which creates a capture group and .*? is a non-greedy moderator and only extracts the quotes and not the text between the quotes. The output is somewhat what we imagined. The sentences are extracted from the file we have pulled. It has specifically returned direct qoutes from the text.There were some few encoding errors which we noticed after taking a deeper look at the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2f60811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c8fc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5248d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Honestly, it feels like we're living our worst nightmare right now,\"]\n",
      "['I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.']\n",
      "['the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.']\n",
      "['in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.']\n",
      "[\"We're not giving up, but it feels really overwhelming to think about what this means and what they're trying to do to us right now,\"]\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# for instance to a text file\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2cc76",
   "metadata": {},
   "source": [
    "I have looked at the data in the notepad that is stored in my desktop. The code did capture most the direct qoutes from the text. There are some direct codes which are questions and the code did not return those direct qoutes. Moreover, there is no further information on who said what qoutes. So, it is a bit hard to follow the storyline since the system only extracted direct codes from the file. I think there is some encoding issues which are not reading the file but it is not returning all the direct qoutes. It can be observed that direct codes in the file which ends with .\". symbol seems to be returned with the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fd4da",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7af966",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by Dr. W.J.B. Mattingly, http://spacy.pythonhumanities.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486724b",
   "metadata": {},
   "source": [
    "Spacy's matcher produces a rules based matching engine and it operated based on tokens which can be accessed within the text. All matchers are processed using patterns which are already defined by the matcher. In this approach we first import the Matcher from spacy.matcher. Then we initialize the matcher Matcher(nlp.vocab) with the shared vocabulary using the document the code will operate on. This apporoach will process the texts, then find quotes and speakers from the data. Similarly like Approach 1 we had to load the text files using the proper encouding file and we input text = f.read () to order the code to read the text. Then we convert the text into a spacy doc in order to apply patterns or attributions. For instance, now we call matcher.add() with a pattern and an ID. In this case we are inputting pattern_n, part-of-speech tag (POS) more specifically Proper_Nouns so that we can find out who the speakers are of the found qoutes. We want to look at the proper nouns in the text.\n",
    "\n",
    "The matcher returns a list of (match, doc[match[1]:match[2]]). Additionally, the match id(POS, PROPN) is the hash value of the string to get the string value of all the proper nouns from the documents. We read in the chapter that spaCy uses a hash function to calculate the hash based on a word string. The second output 1,2 2,3 6,7 is processing the string of the match id and maps to the span of the original document. \n",
    "\n",
    "The output returned based on what we imagined. However, we were still a bit surprised because we thought that the code will only return names of persons but we observed that it returned different entities like countries, cities, institutions etc. It did just capture all sorts of nouns. Furthermore, we observed that the match does not seem to return anything else except the assigned labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d449b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the stuff we'll need\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f709",
   "metadata": {},
   "source": [
    "## spaCy's Matcher\n",
    "This notebook relies on spaCy's Matcher (see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777c3ff",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f91ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file. Remember, you have to do 5\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb581bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0692d",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a08ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "(3232560085755078826, 1, 2) CTV\n",
      "(3232560085755078826, 2, 3) Vancouver\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "(3232560085755078826, 42, 43) Kim\n",
      "(3232560085755078826, 44, 45) Clark\n",
      "(3232560085755078826, 45, 46) Moran\n",
      "(3232560085755078826, 52, 53) Immigration\n",
      "(3232560085755078826, 54, 55) Refugees\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a046e",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c571f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 115, 133) \"The fact that we are being accused right now of an unethical adoption is crazy.\"\n",
      "(16432004385153140588, 164, 174) \"It does say that in the letter,\"\n",
      "(16432004385153140588, 179, 209) \"I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.\"\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c38e6",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project and you'll see a notebook there (quote_extractor_notebook.ipynb)\n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1/data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253777b2",
   "metadata": {},
   "source": [
    "## Approach 1: More Observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bb6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b235dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I came all the way from Syria, and I came here to thank you,']\n",
      "[\"When we're faced with anxieties, it's very easy to have those fears drummed up and exacerbated — getting people to point fingers and lay blame,\"]\n"
     ]
    }
   ],
   "source": [
    "#TEXT 2\n",
    "with open (\"data/5c4a89f31e67d78e27233c5d.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents = find_sents(text)\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3421d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Gasol] is one of the best fives in the league for a long time now, perennial all-star, defensive player of the year,']\n",
      "['Frustration, anger and helplessness']\n",
      "[\"I'm back.\"]\n"
     ]
    }
   ],
   "source": [
    "#TEXT 3\n",
    "with open (\"data/5c5d37341e67d78e275dc30f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents = find_sents(text)\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62924e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to date, we have not yet identified any issues regarding police officer conduct.']\n",
      "['chaos all around.']\n",
      "['unpredictable']\n",
      "['the level of trust between the RCMP and the Hereditary Chiefs now in place will continue to play a direct and positive role going forward.']\n",
      "[\"we're not there yet.\"]\n",
      "['I believe there has to be trust built.']\n",
      "['significantly scaled down presence, one that everyone is comfortable with.']\n"
     ]
    }
   ],
   "source": [
    "#TEXT 4\n",
    "with open (\"data/5c3e11b11e67d78e27f2357a.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents = find_sents(text)\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c4fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"We're not considering changing something in the legislation,\"]\n",
      "[\"We're confident in the legislation that we brought forward, that it finds the right balance in terms of being able to access medical assistance in dying, protecting the autonomy of individuals to make the appropriate decisions for themselves as well as protecting vulnerable individuals.\"]\n",
      "[\"Our hope is for the country that the new justice minister will follow through on what he's already said and what he already knows that (the law) doesn't do enough for the most vulnerable people,\"]\n",
      "[\"Audrey's amendment,\"]\n",
      "['in an advanced stage of irreversible decline', 'reasonably foreseeable.']\n",
      "['grievous and irremediable']\n"
     ]
    }
   ],
   "source": [
    "#TEXT 5\n",
    "with open (\"data/5c3f4e281e67d78e27f62b50.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents = find_sents(text)\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa100ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm so excited and I am so honoured to be selected by you here,\"]\n",
      "['elected']\n",
      "['Essentially, the leader will choose the candidate in each byelection,']\n",
      "['whole country']\n"
     ]
    }
   ],
   "source": [
    "#TEXT 6\n",
    "with open (\"data/5c2994741e67d78e27b6d2ff.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "def find_sents(text): \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents = find_sents(text)\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84353560",
   "metadata": {},
   "source": [
    "# Approach 2: More Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a51b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"data/5c4a89f31e67d78e27233c5d.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff00fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "(3232560085755078826, 0, 1) MIRAMICHI\n",
      "(3232560085755078826, 2, 3) N.B.\n",
      "(3232560085755078826, 4, 5) Justin\n",
      "(3232560085755078826, 5, 6) Trudeau\n",
      "(3232560085755078826, 44, 45) Thursday\n",
      "(3232560085755078826, 52, 53) New\n",
      "(3232560085755078826, 53, 54) Brunswick\n",
      "(3232560085755078826, 69, 70) Canada\n",
      "(3232560085755078826, 79, 80) Syria\n",
      "(3232560085755078826, 90, 91) Tasmeen\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27eca3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(16432004385153140588, 206, 224) \"Welcoming people who are trying to build a better life is what built this country,\"\n",
      "(16432004385153140588, 707, 719) \"Obviously the Francophonie Games are deeply important to me,\"\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219797cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "(3232560085755078826, 1, 2) Toronto\n",
      "(3232560085755078826, 2, 3) Raptors\n",
      "(3232560085755078826, 15, 16) NBA\n",
      "(3232560085755078826, 20, 21) Marc\n",
      "(3232560085755078826, 21, 22) Gasol\n",
      "(3232560085755078826, 24, 25) Memphis\n",
      "(3232560085755078826, 25, 26) Grizzlies\n",
      "(3232560085755078826, 27, 28) Jonas\n",
      "(3232560085755078826, 28, 29) Valanciunas\n",
      "(3232560085755078826, 30, 31) C.J.\n",
      "2\n",
      "(16432004385153140588, 137, 142) \" VanVleet said. \"\n",
      "(16432004385153140588, 697, 702) \" said Powell. \"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"data/5c5d37341e67d78e275dc30f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27207c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "(3232560085755078826, 0, 1) RCMP\n",
      "(3232560085755078826, 24, 25) B.C.\n",
      "(3232560085755078826, 29, 30) Gidimt'en\n",
      "(3232560085755078826, 40, 41) Wet'suwet'en\n",
      "(3232560085755078826, 49, 50) Coastal\n",
      "(3232560085755078826, 50, 51) GasLink\n",
      "(3232560085755078826, 65, 66) Prince\n",
      "(3232560085755078826, 66, 67) George\n",
      "(3232560085755078826, 68, 69) B.C.\n",
      "(3232560085755078826, 75, 76) December\n",
      "4\n",
      "(16432004385153140588, 250, 256) \"chaos all around.\"\n",
      "(16432004385153140588, 398, 401) \"unpredictable\"\n",
      "(16432004385153140588, 757, 785) \"the level of trust between the RCMP and the Hereditary Chiefs now in place will continue to play a direct and positive role going forward.\"\n",
      "(16432004385153140588, 800, 811) \"I believe there has to be trust built.\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"data/5c3e11b11e67d78e27f2357a.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cef07e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "(3232560085755078826, 0, 1) OTTAWA\n",
      "(3232560085755078826, 5, 6) David\n",
      "(3232560085755078826, 6, 7) Lametti\n",
      "(3232560085755078826, 8, 9) Canada\n",
      "(3232560085755078826, 22, 23) Trudeau\n",
      "(3232560085755078826, 35, 36) Lametti\n",
      "(3232560085755078826, 41, 42) Liberal\n",
      "(3232560085755078826, 58, 59) Canadians\n",
      "(3232560085755078826, 83, 84) Facebook\n",
      "(3232560085755078826, 87, 88) Montreal\n",
      "4\n",
      "(16432004385153140588, 814, 827) \"The real question is do we want more Audrey Parkers?\"\n",
      "(16432004385153140588, 882, 891) \"in an advanced stage of irreversible decline\"\n",
      "(16432004385153140588, 907, 912) \"reasonably foreseeable.\"\n",
      "(16432004385153140588, 935, 940) \"grievous and irremediable\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"data/5c3f4e281e67d78e27f62b50.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "369bc519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "(3232560085755078826, 0, 1) BURNABY\n",
      "(3232560085755078826, 2, 3) B.C.\n",
      "(3232560085755078826, 6, 7) Liberal\n",
      "(3232560085755078826, 7, 8) party\n",
      "(3232560085755078826, 20, 21) NDP\n",
      "(3232560085755078826, 22, 23) Jagmeet\n",
      "(3232560085755078826, 23, 24) Singh\n",
      "(3232560085755078826, 29, 30) British\n",
      "(3232560085755078826, 30, 31) Columbia\n",
      "(3232560085755078826, 33, 34) Karen\n",
      "3\n",
      "(16432004385153140588, 126, 142) \"My eyes are full of tears because I love this land so deeply,\"\n",
      "(16432004385153140588, 444, 447) \"elected\"\n",
      "(16432004385153140588, 645, 649) \"whole country\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"data/5c2994741e67d78e27b6d2ff.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e493ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
