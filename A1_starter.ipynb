{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e97487",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. \n",
    "The binder contains this notebook and some sample files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4e74a",
   "metadata": {},
   "source": [
    "Group 1: Antanila, Rachel, Lovely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd654f4",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c548f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5207a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facf0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b25ae3",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e6dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9211596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e54193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Honestly, it feels like we're living our worst nightmare right now,\"]\n",
      "['It does say that in the letter,', 'I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.']\n",
      "['the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.']\n",
      "['in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.']\n",
      "[\"We're not giving up, but it feels really overwhelming to think about what this means and what they're trying to do to us right now,\"]\n",
      "[\"I can't believe that this is our life, that this is our story.\"]\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# for instance to a text file\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c6a71",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92488d32",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by Dr. W.J.B. Mattingly, http://spacy.pythonhumanities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c740d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the stuff we'll need\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411eae45",
   "metadata": {},
   "source": [
    "## spaCy's Matcher\n",
    "This notebook relies on spaCy's Matcher (see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed7cac",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29bad1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file. Remember, you have to do 5\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f969ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CTV Vancouver. \n",
      " An Abbotsford, B.C. couple that has been waiting nearly two years to bring their newly adopted son home from Africa has learned that the Canadian government is not prepared to grant the child citizenship. \n",
      " Kim and Clark Moran received a letter this week from Immigration, Refugees and Citizenship Canada informing them that the federal department has concerns about two-year-old Ayo, whom the couple claims they adopted from an orphanage in Nigeria and gained custody of in August. \n",
      " \"Honestly, it feels like we're living our worst nightmare right now,\" Kim told CTV News Friday. \"The fact that we are being accused right now of an unethical adoption is crazy.\". \n",
      " CTV News has learned that a third party has come forward with an allegation that Ayo's adoption came from a private residence and not an orphanage. \n",
      " \"It does say that in the letter,\" Kim confirmed, adding that \"I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.\". \n",
      " Kim would not share a copy of the letter with CTV. \n",
      " In a statement, IRCC did not reveal any details regarding the Ayo's paperwork, but said \"the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.\". \n",
      " The department added that \"in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.\". \n",
      " The Morans' story first made headlines in October, some three months after they first travelled to Africa to finalize the adoption. \n",
      " According to Kim, the Canadian high commission in Nigeria doesn't have an immigration office, so all adoptions out of that country have to be processed in Ghana. \n",
      " Thinking that the immigration papers that would allow them to bring Ayo back to B.C. wouldn't take more than a few weeks to be finalized, Clark headed back to Canada while Kim and their son remained in Accra. \n",
      " It would be months, however, before the family got an update. \n",
      " Kim was forced to switch places with Clark late last month after her multiple sclerosis flared up. \n",
      " He is now in Ghana with Ayo and she is back in B.C., but it's unclear what the family's future will look like. \n",
      " \"If Canada doesn't grant him citizenship, what happens? They send him back to Nigeria to an orphanage? They take him from us even though he's legally our son?\" Kim said. \n",
      " IRCC has told CTV that under ideal circumstances, the two-step adoption and immigration process can take between six and eight months, but that timeframe can be as long as two years depending on the child's country of origin and other factors. \n",
      " The Morans' paperwork was received in February 2017. A year and 10 months later, the family's adoption process is at the top end of the government's estimated timeline, but has not exceeded it. \n",
      " The Morans have 60 days to respond to the letter, and the IRCC stressed Friday that a final decision on Ayo's citizenship application has not yet been made. \n",
      " Kim said the family is working with an immigration lawyer as well as the adoption agency, whose name she would not reveal, as they weigh their options. \n",
      " \"We're not giving up, but it feels really overwhelming to think about what this means and what they're trying to do to us right now,\" she said. \n",
      " \"I can't believe that this is our life, that this is our story.\". \n",
      " With files from CTV Vancouver's Ben Miljure\n"
     ]
    }
   ],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#show the text\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749f943",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86fedc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "(3232560085755078826, 111, 114) CTV News Friday\n",
      "(3232560085755078826, 1, 3) CTV Vancouver\n",
      "(3232560085755078826, 44, 46) Clark Moran\n",
      "(3232560085755078826, 56, 58) Citizenship Canada\n",
      "(3232560085755078826, 135, 137) CTV News\n",
      "(3232560085755078826, 722, 724) CTV Vancouver\n",
      "(3232560085755078826, 725, 727) Ben Miljure\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "amount of matches: 4\n",
      "(3232560085755078826, 241, 242) said\n",
      "(3232560085755078826, 509, 510) said\n",
      "(3232560085755078826, 634, 635) said\n",
      "(3232560085755078826, 696, 697) said\n"
     ]
    }
   ],
   "source": [
    "## Extract multi-word nouns \n",
    "## greedy = \"LONGEST\" will match as much as possible of the noun, in this case, do we want \"CTV News Friday\" or just Friday?\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# Dependency Parse: https://spacy.io/usage/visualizers\n",
    "# https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "# spaCy matcher documentation: https://spacy.io/api/matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_z = [{\"TEXT\": \"said\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_z], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (\"amount of matches:\", len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In this text, some indirect examples of quotes are According to [person],\n",
    "# IRCC has told, IRCC has stressed, Kim said, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba7ee4",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f857916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 115, 133) \"The fact that we are being accused right now of an unethical adoption is crazy.\"\n",
      "(16432004385153140588, 164, 174) \"It does say that in the letter,\"\n",
      "(16432004385153140588, 179, 209) \"I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.\"\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9e0bd",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project and you'll see a notebook there (quote_extractor_notebook.ipynb)\n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1/data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115a0d8",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925ebf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
